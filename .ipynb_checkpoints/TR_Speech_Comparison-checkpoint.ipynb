{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing TR Speeches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import operator as opr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading in the speeches\n",
    "The 1901 and 1905 speeches are inaugural addresses while the others are State of the Union speeches submitted in writing to Congress.\n",
    "\n",
    "The speeches were read from <a href=https://www.presidency.ucsb.edu/documents/presidential-documents-archive-guidebook/annual-messages-congress-the-state-the-union> The American Presidency Project website </a> and copied to .txt files saved locally using the PullSpeechURLs.py script. This was conducted in early 2018 and the site has since changed its format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "TR1901= open(r'Speeches\\TR_December_3_1901.txt','r').read().lower()\n",
    "TR1902= open(r'Speeches\\TR_December_2_1902.txt','r').read().lower()\n",
    "TR1903= open(r'Speeches\\TR_December_7_1903.txt','r').read().lower()\n",
    "TR1904= open(r'Speeches\\TR_December_6_1904.txt','r').read().lower()\n",
    "TR1905= open(r'Speeches\\TR_December_5_1905.txt','r').read().lower()\n",
    "TR1906= open(r'Speeches\\TR_December_3_1906.txt','r').read().lower()\n",
    "TR1907= open(r'Speeches\\TR_December_3_1907.txt','r').read().lower()\n",
    "TR1908= open(r'Speeches\\TR_December_8_1908.txt','r').read().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRspeeches= [TR1901,TR1902,TR1903,TR1904,TR1905,TR1906,TR1907,TR1908]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Stopwords and start tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS= set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "TR_Tok= [word_tokenize(i) for i in TRspeeches]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20944\n",
      "10461\n",
      "16322\n",
      "18731\n",
      "26999\n",
      "25511\n",
      "29518\n",
      "21052\n"
     ]
    }
   ],
   "source": [
    "for i in TR_Tok:\n",
    "    print(len(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "TR_NoStop= [[i for i in j if i not in STOPWORDS] for j in TR_Tok]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "TR_Sort= [sorted(list(i)) for i in TR_NoStop]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The punctuation pulling requires a little refining. This process is very manual currently; I am searching for the index of the first proper word and taking a slice from the beginning of the sorted text to the index of the character which precedes the first word.\n",
    "\n",
    "Issues:\n",
    "\n",
    "Split words (hyphenated and continued on another line) may be excluded (\"-ire\")\n",
    "\n",
    "Typos cause certain words, though not overly consequential ones, to be removed as a result as some will have a \".\" attached on the left.\n",
    "\n",
    "Unless much more manual processing is done, numbers are excluded. Nautical measurements Teddy uses (\"20-knot\", etc) are excluded as well, but these may provide interesting insight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "TR_PuncInd= [TR_Sort[0][0:1501],TR_Sort[1][0:776],TR_Sort[2][0:1631],TR_Sort[3][0:1443],TR_Sort[4][0:2063],TR_Sort[5][0:2062],TR_Sort[6][0:2351],TR_Sort[7][0:1770]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining function remove_punc() to loop through each item in a list (TR_PuncInd) to then iterate over items in the sublists. Initializing TR_Punc as empty list to be appended by remove_punc()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "TR_Punc= []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punc(arg):\n",
    "    for i in arg:\n",
    "        if i not in TR_Punc:\n",
    "            TR_Punc.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in TR_PuncInd:\n",
    "    remove_punc(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect all the words into a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "words= []\n",
    "for i in range(0,len(TR_Tok)):\n",
    "    words.append(list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_words(arg,num):\n",
    "    for i in arg:\n",
    "        if i not in words[num]:\n",
    "            if i not in TR_Punc:\n",
    "                words[num].append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(TR_Sort)):\n",
    "    add_words(TR_Sort[i],i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect all the counts for each word to a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_count(arg,num):\n",
    "    for i in arg:\n",
    "        if i in words[num]:\n",
    "            count[num][words[num].index(i)]= count[num][words[num].index(i)] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "count= [[0 for i in range(0,len(words[j]))] for j in range(0,len(words))]\n",
    "\n",
    "for i in range(0,len(TR_Sort)):\n",
    "    add_count(TR_Sort[i],i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "TR_Max= [max(i) for i in count]\n",
    "TR_Min= [min(i) for i in count]\n",
    "TR_Mean= [sum(i)/len(i) for i in count]\n",
    "\n",
    "count_sort= [sorted(list(i)) for i in count]\n",
    "TR_Median= [[i[int(len(i)/2-.5)],i[int(len(i)/2+.5)]] for i in count_sort]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zip up the lists to create the Bag of Words for each speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "BOW= [list(zip(words[i],count[i])) for i in range(0,len(words))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An interesting preview into the upcoming analysis. TR's speeches seem to get more repetitive over time, with his most repetitive (by far) being 1907. This is the only speech I've seen with a median greater than 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('abandon', 1), ('abandoning', 1), ('abide', 1), ('abilities', 1), ('ability', 4), ('ability.i', 1), ('able', 11), ('ablest', 1), ('aboard', 2), ('abolished', 1), ('aboriginal', 1), ('abounding', 1), ('about.i', 1), ('abra', 1), ('abroad', 7), ('abrogated', 1), ('absolutely', 4), ('abundance', 1), ('abundant', 1), ('abuses', 3)] \n",
      " \tMax:  60 \n",
      " \tMin:  1 \n",
      " \tMean:  2.7954815695600477 \n",
      " \tMedian:  [1, 1] \n",
      "\n",
      "[('abandon', 1), ('abandoning', 1), ('abandonment', 1), ('ability', 2), ('able', 1), ('aboard', 1), ('abroad', 3), ('abroad.i', 1), ('absolute', 1), ('absolutely', 1), ('absorption', 4), ('abstract', 1), ('acceded', 1), ('accepted', 1), ('access', 3), ('accidental', 1), ('accomplish', 1), ('accomplished', 4), ('accord', 1), ('accordance', 1)] \n",
      " \tMax:  36 \n",
      " \tMin:  1 \n",
      " \tMean:  2.214935375777884 \n",
      " \tMedian:  [1, 1] \n",
      "\n",
      "[('abandon', 2), ('abandoned', 1), ('abating', 1), ('ability', 2), ('abnormal', 2), ('above-mentioned', 1), ('absence', 1), ('absolute', 1), ('accentuated', 1), ('accepted', 1), ('access', 1), ('accomplish', 3), ('accomplished', 3), ('accomplishes', 1), ('accomplishment', 1), ('accordance', 5), ('accorded', 1), ('according', 1), ('accordingly', 2), ('account', 1)] \n",
      " \tMax:  74 \n",
      " \tMin:  1 \n",
      " \tMean:  2.6604870956015993 \n",
      " \tMedian:  [1, 1] \n",
      "\n",
      "[('abandon', 1), ('abeyance', 1), ('ability', 1), ('abject', 1), ('able', 6), ('aboard', 1), ('abolished', 1), ('abolition', 1), ('abroad', 7), ('absence', 1), ('absolute', 1), ('absolutely', 3), ('absurdity', 2), ('abundant', 2), ('abundantly', 1), ('abuses', 5), ('accept', 4), ('accessible', 1), ('accessible.second', 1), ('accidents', 4)] \n",
      " \tMax:  57 \n",
      " \tMin:  1 \n",
      " \tMean:  2.6855867346938775 \n",
      " \tMedian:  [1, 1] \n",
      "\n",
      "[('abdicating', 1), ('abdication', 1), ('ability', 3), ('able', 15), ('aboard', 2), ('abode', 2), ('abolished', 4), ('abolishing', 1), ('abolishment', 1), ('abroad', 3), ('absence', 1), ('absolute', 2), ('absolutely', 2), ('absurd', 2), ('abuse', 2), ('abused', 1), ('abuses', 7), ('abutting', 1), ('accept', 1), ('acceptance', 1)] \n",
      " \tMax:  87 \n",
      " \tMin:  1 \n",
      " \tMean:  3.2375033774655497 \n",
      " \tMedian:  [1, 1] \n",
      "\n",
      "[('abandon', 1), ('ability', 2), ('able', 7), ('ably', 1), ('abnormal', 1), ('aboard', 1), ('abolished', 1), ('abolishing', 1), ('abolition', 1), ('abolition.it', 1), ('abominable', 2), ('abreast', 1), ('absolute', 2), ('absolutely', 4), ('absurd', 3), ('absurdity', 2), ('abundantly', 1), ('abuse', 5), ('abused', 1), ('abuses', 4)] \n",
      " \tMax:  81 \n",
      " \tMin:  1 \n",
      " \tMean:  3.1093707062379776 \n",
      " \tMedian:  [1, 1] \n",
      "\n",
      "[('abandonment', 2), ('abdicating', 1), ('ability', 8), ('able', 8), ('ably', 1), ('abnormal', 1), ('abolished', 2), ('abroad', 4), ('absolute', 2), ('absolutely', 3), ('absurd', 1), ('absurdity', 2), ('abuse', 8), ('abuses', 5), ('academic', 1), ('academy', 7), ('accept', 1), ('acceptance', 3), ('accepted', 1), ('accepting', 1)] \n",
      " \tMax:  100 \n",
      " \tMin:  1 \n",
      " \tMean:  3.4057557687321753 \n",
      " \tMedian:  [2, 2] \n",
      "\n",
      "[('abandon', 3), ('abandoning', 1), ('abandonment', 1), ('ability', 1), ('able', 13), ('abolition', 2), ('abounding', 1), ('abroad', 1), ('absence', 3), ('absentee', 1), ('absolute', 2), ('absolutely', 9), ('absorbed', 1), ('abstract', 1), ('absurd', 2), ('abundance', 1), ('abuse', 2), ('abuses', 5), ('academic', 2), ('academy', 1)] \n",
      " \tMax:  57 \n",
      " \tMin:  1 \n",
      " \tMean:  2.9181761399125548 \n",
      " \tMedian:  [1, 1] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(0,len(BOW)):\n",
    "    print(BOW[i][0:20],'\\n','\\tMax: ',TR_Max[i],'\\n','\\tMin: ',TR_Min[i],'\\n','\\tMean: ',TR_Mean[i],'\\n','\\tMedian: ',TR_Median[i],'\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
